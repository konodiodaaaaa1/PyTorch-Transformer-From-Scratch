import torch
import torch.nn as nn
import torch.amp as amp
import torch.nn.functional as F
from torch.utils.data import DataLoader, Dataset
from torch.nn.utils.rnn import pad_sequence
from datasets import load_dataset
from tokenizers import Tokenizer
from pathlib import Path
import sys
import os
import math
from typing import Callable

# --- ç¡®ä¿èƒ½æ‰¾åˆ°æˆ‘ä»¬çš„æ¨¡å— ---
sys.path.append(str(Path(__file__).parent))
try:
    from model import Transformer
    from train_utils import create_padding_mask, create_look_ahead_mask
except ImportError as e:
    print(f"é”™è¯¯: æ‰¾ä¸åˆ° model.py æˆ– train_utils.pyã€‚ {e}")
    sys.exit(1)

os.environ["TOKENIZERS_PARALLELISM"] = "false"

# ----------------------------------------------------------------------
# ã€å…¨å±€ã€‘: åˆ†è¯å™¨å’Œç‰¹æ®Š Token
# ----------------------------------------------------------------------

print("æ­£åœ¨åŠ è½½åˆ†è¯å™¨...")
SCRIPT_DIR = Path(__file__).parent
SRC_TOKENIZER_PATH = SCRIPT_DIR / "src_tokenizer.json"
TGT_TOKENIZER_PATH = SCRIPT_DIR / "tgt_tokenizer.json"

src_tokenizer = Tokenizer.from_file(str(SRC_TOKENIZER_PATH))
tgt_tokenizer = Tokenizer.from_file(str(TGT_TOKENIZER_PATH))

PAD_IDX_SRC = src_tokenizer.token_to_id("<pad>")
PAD_IDX_TGT = tgt_tokenizer.token_to_id("<pad>")
SOS_IDX_TGT = tgt_tokenizer.token_to_id("<sos>")
EOS_IDX_TGT = tgt_tokenizer.token_to_id("<eos>")

assert PAD_IDX_SRC == PAD_IDX_TGT, "Pad ID å¿…é¡»ç›¸åŒ (é€šå¸¸ä¸º 0)"
SRC_VOCAB_SIZE = src_tokenizer.get_vocab_size()
TGT_VOCAB_SIZE = tgt_tokenizer.get_vocab_size()


# ----------------------------------------------------------------------
# ã€æ•°æ®ç®¡é“ã€‘: (æ¥è‡ª train.py)
# ----------------------------------------------------------------------

class TranslationDataset(Dataset):
    def __init__(self, data, src_tokenizer, tgt_tokenizer):
        self.data = data
        self.src_tokenizer = src_tokenizer
        self.tgt_tokenizer = tgt_tokenizer

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        sample = self.data[idx]
        src_text = sample['classical']
        tgt_text = sample['modern']

        src_ids = self.src_tokenizer.encode(src_text).ids
        tgt_ids = self.tgt_tokenizer.encode(tgt_text).ids

        tgt_input = torch.tensor([SOS_IDX_TGT] + tgt_ids, dtype=torch.long)
        tgt_label = torch.tensor(tgt_ids + [EOS_IDX_TGT], dtype=torch.long)
        src_input = torch.tensor(src_ids, dtype=torch.long)

        return {
            "src_input": src_input,
            "tgt_input": tgt_input,
            "tgt_label": tgt_label
        }


def setup_dataloaders(
        train_data,
        val_data,
        src_tokenizer: Tokenizer,
        tgt_tokenizer: Tokenizer,
        batch_size: int,
        max_allowed_len: int
):
    print("  æ­£åœ¨æ„å»º Torch Dataset...")

    def filter_long(example):
        return len(example['classical']) < max_allowed_len and \
            len(example['modern']) < max_allowed_len

    print(f"  è¿‡æ»¤å‰ (Train): {len(train_data)} / (Val): {len(val_data)}")
    train_data = train_data.filter(filter_long)
    val_data = val_data.filter(filter_long)
    print(f"  è¿‡æ»¤å (Train): {len(train_data)} / (Val): {len(val_data)}")

    train_dataset_torch = TranslationDataset(train_data, src_tokenizer, tgt_tokenizer)
    val_dataset_torch = TranslationDataset(val_data, src_tokenizer, tgt_tokenizer)

    def collate_fn(batch: list):
        src_batch = pad_sequence(
            [item['src_input'] for item in batch],
            batch_first=True, padding_value=PAD_IDX_SRC
        )
        tgt_inputs = pad_sequence(
            [item['tgt_input'] for item in batch],
            batch_first=True, padding_value=PAD_IDX_TGT
        )
        tgt_labels = pad_sequence(
            [item['tgt_label'] for item in batch],
            batch_first=True, padding_value=PAD_IDX_TGT
        )
        return src_batch, tgt_inputs, tgt_labels

    train_loader = DataLoader(
        train_dataset_torch,
        batch_size=batch_size,
        shuffle=True,  # (åœ¨è¿‡æ‹Ÿåˆæµ‹è¯•ä¸­, shuffle æ„ä¹‰ä¸å¤§ä½†æ— å®³)
        collate_fn=collate_fn,
        num_workers=0,
        pin_memory=True
    )
    val_loader = DataLoader(
        val_dataset_torch,
        batch_size=batch_size,
        shuffle=False,
        collate_fn=collate_fn,
        num_workers=0,
        pin_memory=True
    )
    return train_loader, val_loader


# ----------------------------------------------------------------------
# ã€æ¨¡å‹ä¸ä¼˜åŒ–å™¨ã€‘: (æ¥è‡ª train.py, ä¿æŒå¤§æ ·æœ¬é…ç½®)
# ----------------------------------------------------------------------
def initialize_model(device: torch.device, checkpoint_path: str = None):
    # (!!!) ä½¿ç”¨æ‚¨ train.py ä¸­çš„å¤§æ ·æœ¬é…ç½® (!!!)
    D_MODEL = 512
    NUM_LAYERS = 6
    NUM_HEADS = 8
    D_FF = 2048
    MAX_LEN = 5000
    DROPOUT = 0.0  # (!!!) ä¿æŒ Dropout > 0 (!!!)

    model = Transformer(
        SRC_VOCAB_SIZE, TGT_VOCAB_SIZE, D_MODEL,
        NUM_LAYERS, NUM_HEADS, D_FF, MAX_LEN, DROPOUT
    ).to(device)

    loss_fn = nn.CrossEntropyLoss(ignore_index=PAD_IDX_TGT, label_smoothing=0.0)

    # (!!!) ä¿æŒ train.py ä¸­çš„ Warmup é…ç½® (!!!)
    optimizer = torch.optim.Adam(model.parameters(), lr=5e-3, betas=(0.9, 0.98), eps=1e-9)
    lr_scheduler = None
    warmup_steps = 2000
    d_model_inv_sqrt = D_MODEL ** (-0.5)

    def lr_scheduler_lambda(step_num):
        step_num += 1
        arg1 = step_num ** (-0.5)
        arg2 = step_num * (warmup_steps ** (-1.5))
        lr_scale_factor = 0.8  # (æ¥è‡ª train.py)
        return d_model_inv_sqrt * min(arg1, arg2) * lr_scale_factor

    # lr_scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lr_scheduler_lambda)

    start_epoch = 1
    best_val_loss = float('inf')

    if checkpoint_path and os.path.exists(checkpoint_path):
        print(f"--- æ­£åœ¨ä» {checkpoint_path} åŠ è½½æ£€æŸ¥ç‚¹... ---")
        checkpoint = torch.load(checkpoint_path, map_location=device)
        model.load_state_dict(checkpoint['model_state_dict'])
        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        start_epoch = checkpoint['epoch'] + 1
        best_val_loss = checkpoint.get('best_val_loss', float('inf'))
        if 'scheduler_state_dict' in checkpoint:
            lr_scheduler.load_state_dict(checkpoint['scheduler_state_dict'])
        print(f"--- å·²åŠ è½½ã€‚å°†ä» Epoch {start_epoch} å¼€å§‹è®­ç»ƒã€‚---")
    else:
        print(f"--- æœªæ‰¾åˆ°æ£€æŸ¥ç‚¹ã€‚æ­£åœ¨ä»å¤´å¼€å§‹è®­ç»ƒ... ---")

        print("--- (æ­£åœ¨åº”ç”¨æ ‡å‡† Transformer åˆå§‹åŒ–: Xavier Uniform) ---")
        for name, p in model.named_parameters():
            if p.dim() > 1:
                nn.init.xavier_uniform_(p)

    print(f"æ¨¡å‹å·²åœ¨ {device} ä¸Šåˆå§‹åŒ– (Dropout={DROPOUT})")
    return model, loss_fn, optimizer, start_epoch, best_val_loss, lr_scheduler


# ----------------------------------------------------------------------
# ã€è¯Šæ–­å‡½æ•°ã€‘: (æ¥è‡ª debug_app.py)
# ----------------------------------------------------------------------

def test_autoregressive_steps(
        model: nn.Module,
        src_tensor: torch.Tensor,
        tgt_tensor: torch.Tensor,  # å®Œæ•´çš„çœŸå®ç›®æ ‡ [SOS, w1, w2, ..., EOS]
        src_mask: torch.Tensor,
        pad_idx_tgt: int,
        create_look_ahead_mask_fn: Callable,
        device: torch.device
):
    """
    (!!!) å…³é”®è¯Šæ–­ï¼šé€æ—¶é—´æ­¥æ£€æŸ¥ (æ¥è‡ª debug_app.py) (!!!)
    """
    print("\n" + "=" * 70)
    print("ğŸ”¬ é€æ—¶é—´æ­¥ è‡ªå›å½’é¢„æµ‹æ£€æŸ¥ (Autoregressive Step Test)")
    print(f"  (!!!) å½“å‰æ¨¡å‹æ¨¡å¼: {'TRAIN' if model.training else 'EVAL'} (!!!)")
    print("=" * 70)

    # (æ­¤å‡½æ•°å·²åŒ…å« model.eval()ï¼Œä½†æˆ‘ä»¬åœ¨å¤–éƒ¨è°ƒç”¨æ—¶ä¼šè¦†ç›–å®ƒ)

    if src_tensor.shape[0] != 1 or tgt_tensor.shape[0] != 1:
        print(f"  [è­¦å‘Š] æ­¤æµ‹è¯•å‡½æ•°è®¾è®¡ä¸º batch_size=1ã€‚æ£€æµ‹åˆ° BS={src_tensor.shape[0]}ã€‚")
        print("         å°†å¼ºåˆ¶ä½¿ç”¨ç¬¬ä¸€ä¸ªæ ·æœ¬ (index 0) è¿›è¡Œæµ‹è¯•ã€‚")
        src_tensor = src_tensor[:1]
        tgt_tensor = tgt_tensor[:1]
        src_mask = src_mask[:1]

    true_tokens = tgt_tensor[0]
    max_T = true_tokens.shape[0]
    correct_predictions = 0
    total_predictions = max_T - 1

    if total_predictions <= 0:
        print("  [é”™è¯¯] ç›®æ ‡å¼ é‡å¤ªçŸ­ï¼Œæ— æ³•è¿›è¡Œæµ‹è¯•ã€‚")
        return

    print(f"  å°†æµ‹è¯• {total_predictions} ä¸ªæ—¶é—´æ­¥ (T=1 åˆ° T={total_predictions}).")
    print(f"  å®Œæ•´çœŸå®åºåˆ—: {true_tokens.cpu().numpy().tolist()}")
    print("-" * 70)

    try:
        # (!!!) æˆ‘ä»¬ä½¿ç”¨ torch.no_grad() æ¥é˜²æ­¢æ¢¯åº¦è®¡ç®—,
        # (!!!) ä½†æ¨¡å‹ *æœ¬èº«* çš„æ¨¡å¼ (train/eval) æ˜¯ç”±å¤–éƒ¨æ§åˆ¶çš„
        with torch.no_grad(), torch.amp.autocast(device_type=device.type, enabled=(device.type == 'cuda')):
            # 1. ç¼–ç å™¨åªè¿è¡Œä¸€æ¬¡
            # (æˆ‘ä»¬å¿…é¡»ç¡®ä¿ model.encoder å­˜åœ¨)
            enc_output = model.encoder(src_tensor, src_mask)

            # 2. è¿­ä»£ T=1 åˆ° T_max-1
            for T in range(1, max_T):
                current_tgt_input = tgt_tensor[:, :T]
                current_tgt_mask = create_look_ahead_mask_fn(current_tgt_input, pad_idx_tgt).to(device)

                # (æˆ‘ä»¬å¿…é¡»ç¡®ä¿ model.decoder å­˜åœ¨)
                output_logits_full = model.decoder(current_tgt_input, enc_output, current_tgt_mask, src_mask)

                # (æˆ‘ä»¬å¿…é¡»ç¡®ä¿ model.final_proj å­˜åœ¨)
                output_logits = model.final_proj(output_logits_full)

                pred_logits_at_T = output_logits[:, -1, :]
                pred_token_id = pred_logits_at_T.argmax().item()
                true_token_id = true_tokens[T].item()

                is_correct = (pred_token_id == true_token_id)
                if is_correct:
                    correct_predictions += 1

                status_icon = "âœ…" if is_correct else "âŒ"
                input_seq_str = current_tgt_input[0].cpu().numpy().tolist()

                print(f"  {status_icon} [T={T:02d}] è¾“å…¥: {input_seq_str}")
                print(f"         é¢„æµ‹: {pred_token_id:5d}  |  çœŸå®: {true_token_id:5d}")

                if not is_correct:
                    # (åªåœ¨ç¬¬ä¸€æ¬¡å¤±è´¥æ—¶æ‰“å°å¹¶è·³å‡º)
                    print(f"         (!! åœ¨ T={T} å¤±è´¥ !!)")
                    break  # (æˆ‘ä»¬åªå…³å¿ƒå®ƒæ˜¯å¦å®Œç¾)

    except Exception as e:
        print(f"\n  [!! ä¸¥é‡é”™è¯¯ !!] æµ‹è¯•åœ¨ T={T} ä¸­æ–­: {e}")
        import traceback
        traceback.print_exc()

    print("\n[åˆ†æç»“æœ]")
    accuracy = (correct_predictions / total_predictions) * 100
    print(f"  æ€»ä½“æ­£ç¡®ç‡: {correct_predictions} / {total_predictions} ({accuracy:.2f}%)")

    if correct_predictions == total_predictions:
        print("  ğŸ‰ å®Œç¾! æ¨¡å‹åœ¨æ‰€æœ‰æ—¶é—´æ­¥å‡é¢„æµ‹æ­£ç¡® (Teacher Forcing æ¨¡å¼)ã€‚")
    else:
        print(f"  ğŸš¨ å¤±è´¥! æ¨¡å‹åœ¨ T={correct_predictions + 1} æ—¶é¦–æ¬¡å‡ºç°é¢„æµ‹é”™è¯¯ã€‚")

    print("=" * 70 + "\n")
    return accuracy  # (è¿”å›å‡†ç¡®ç‡)


def diagnose_model_health(model, src_sentence: str, device):
    """(è¯Šæ–­) T=1 æ•°å€¼åˆ†æ (æ¥è‡ª debug_app.py)"""
    print("\n" + "=" * 60)
    print("ğŸ¥ æ¨¡å‹å¥åº·è¯Šæ–­ (T=1 æ•°å€¼åˆ†æ)")
    print("=" * 60)

    global src_tokenizer, tgt_tokenizer
    global PAD_IDX_SRC, PAD_IDX_TGT, SOS_IDX_TGT, EOS_IDX_TGT

    model.eval()  # (æ­¤è¯Šæ–­æ€»æ˜¯åœ¨ eval æ¨¡å¼ä¸‹è¿è¡Œ)

    src_ids = src_tokenizer.encode(src_sentence).ids
    src_tensor = torch.tensor(src_ids, dtype=torch.long).unsqueeze(0).to(device)
    src_mask = create_padding_mask(src_tensor, PAD_IDX_SRC).to(device)
    tgt_input_T1 = torch.tensor([[SOS_IDX_TGT]], dtype=torch.long, device=device)
    tgt_mask_T1 = create_look_ahead_mask(tgt_input_T1, PAD_IDX_TGT).to(device)

    try:
        with torch.no_grad(), torch.amp.autocast(device_type=device.type, enabled=(device.type == 'cuda')):
            enc_output = model.encoder(src_tensor, src_mask)

            dec_output_T1 = model.decoder(tgt_input_T1, enc_output, tgt_mask_T1, src_mask)
            logits_T1 = model.final_proj(dec_output_T1)
            logits_vec = logits_T1[0, 0]

        print("[1] T=1 å‰å‘ä¼ æ’­æ¨¡æ‹Ÿ (Top 5):")
        probs = F.softmax(logits_vec, dim=0)
        top5_values, top5_indices = torch.topk(logits_vec, k=5)

        print(f"  Logitsç»Ÿè®¡: mean={logits_vec.mean().item():.4f}, std={logits_vec.std().item():.4f}")
        for rank, (idx, val) in enumerate(zip(top5_indices, top5_values), 1):
            word = tgt_tokenizer.decode([idx.item()])
            prob = probs[idx].item()
            print(f"    {rank}. {word:<10} (id={idx.item():<6}) logit={val.item():7.2f} prob={prob:.4f}")

        predicted_id = top5_indices[0].item()
        if predicted_id == EOS_IDX_TGT:
            print(f"\n  (!!!!!!) ç—‡çŠ¶ç¡®è®¤: æ¨¡æ‹Ÿ T=1 é¢„æµ‹äº† [EOS]! (Logit={top5_values[0].item():.2f})")

        return True

    except Exception as e:
        print(f"  [é”™è¯¯] T=1 å‰å‘ä¼ æ’­å¤±è´¥: {e}")
        return False


def translate_greedy(
        model,
        src_sentence: str,
        src_tokenizer,
        tgt_tokenizer,
        device,
        pad_idx_src,
        pad_idx_tgt,
        sos_idx_tgt,
        eos_idx_tgt,
        max_len: int = 150
):
    """(è¯Šæ–­) è´ªå©ªè§£ç  (æ¥è‡ª debug_app.py)"""

    print("\n" + "=" * 60)
    print("âœï¸ æ­£åœ¨è¿è¡Œè´ªå©ªç¿»è¯‘ (Greedy Translate)")
    print(f"  (!!!) å½“å‰æ¨¡å‹æ¨¡å¼: {'TRAIN' if model.training else 'EVAL'} (!!!)")
    print("=" * 60)

    # (æ­¤å‡½æ•°å¼ºåˆ¶ä½¿ç”¨ eval æ¨¡å¼è¿›è¡Œè§£ç )
    model.eval()

    src_ids = src_tokenizer.encode(src_sentence).ids
    src_tensor = torch.tensor(src_ids, dtype=torch.long).unsqueeze(0).to(device)
    src_mask = create_padding_mask(src_tensor, pad_idx_src).to(device)

    try:
        with torch.no_grad(), torch.amp.autocast(device_type=device.type, enabled=(device.type == 'cuda')):
            enc_output = model.encoder(src_tensor, src_mask)
    except Exception as e:
        print(f"  [é”™è¯¯] ç¼–ç å™¨å¤±è´¥: {e}")
        return "[ç¿»è¯‘å¤±è´¥ï¼šç¼–ç å™¨é”™è¯¯]"

    output_ids = [sos_idx_tgt]

    for i in range(max_len):
        tgt_tensor = torch.tensor([output_ids], dtype=torch.long, device=device)
        tgt_mask = create_look_ahead_mask(tgt_tensor, pad_idx_tgt).to(device)

        try:
            with torch.no_grad(), torch.amp.autocast(device_type=device.type, enabled=(device.type == 'cuda')):
                dec_output = model.decoder(tgt_tensor, enc_output, tgt_mask, src_mask)
                logits = model.final_proj(dec_output)
        except Exception as e:
            print(f"  [é”™è¯¯] è§£ç å™¨åœ¨æ­¥éª¤ {i + 1} å¤±è´¥: {e}")
            return f"[ç¿»è¯‘å¤±è´¥ï¼šè§£ç å™¨åœ¨ T={i + 1} é”™è¯¯]"

        next_token = logits[0, -1, :].argmax().item()

        if i == 0:
            print("  [Infer T=1 Debug] Logits Top 5 (æ¥è‡ªè´ªå©ªè§£ç ):")
            top_k_scores, top_k_indices = torch.topk(logits[0, -1, :], 5)
            for k in range(5):
                score = top_k_scores[k].item()
                idx = top_k_indices[k].item()
                word = tgt_tokenizer.decode([idx])
                print(f"    Rank {k + 1}: {word} (id={idx}) -> {score:.2f}")

        word = tgt_tokenizer.decode([next_token])
        print(f"  [Greedy] æ­¥éª¤ {i + 1}: {word} (token_id={next_token})")

        if next_token == eos_idx_tgt:
            break
        output_ids.append(next_token)

    print("=" * 60)

    if len(output_ids) > 1:
        return tgt_tokenizer.decode(output_ids[1:])  # (è·³è¿‡ SOS)
    else:
        return "[ç¿»è¯‘ä¸ºç©º]"


# ----------------------------------------------------------------------
# ã€æ ¸å¿ƒã€‘: è®­ç»ƒå¾ªç¯ (æ¥è‡ª train.py, å·²æ³¨å…¥è¯Šæ–­)
# ----------------------------------------------------------------------
def evaluate(model, dataloader, loss_fn, device, pad_idx_src, pad_idx_tgt):
    """(æ¥è‡ª train.py)"""
    model.eval()
    total_loss = 0
    with torch.no_grad():
        for (src_data, tgt_input_data, tgt_label_data) in dataloader:
            src_batch = src_data.to(device)
            tgt_input = tgt_input_data.to(device)
            tgt_label = tgt_label_data.to(device)
            src_mask = create_padding_mask(src_batch, pad_idx_src).to(device)
            tgt_mask = create_look_ahead_mask(tgt_input, pad_idx_tgt).to(device)

            with torch.amp.autocast(device_type=device.type, enabled=(device.type == 'cuda')):
                logits = model(src_batch, tgt_input, src_mask, tgt_mask)
                loss = loss_fn(
                    logits.reshape(-1, logits.shape[-1]),
                    tgt_label.reshape(-1)
                )
            total_loss += loss.item()
    return total_loss / len(dataloader)


def train_one_epoch(model, train_loader, val_loader, loss_fn, optimizer,
                    device, scaler, pad_idx_src, pad_idx_tgt, epoch, lr_scheduler,
                    log_interval, validate_interval, best_val_loss):
    """
    (!!!) è¿™æ˜¯ train.py çš„ *å¹¶è¡Œ* è®­ç»ƒå¾ªç¯ (!!!)
    (!!!) æˆ‘ä»¬å·²æ³¨å…¥äº† T=1 / T>1 çš„ Loss è¯Šæ–­ (!!!)
    """
    model.train()  # (!!!) å…³é”®ï¼šä¿æŒ .train() æ¨¡å¼ (æ¿€æ´» Dropout)

    total_loss_epoch = 0
    data_iter = iter(train_loader)
    num_batches = len(train_loader)

    for batch_count in range(num_batches):
        try:
            src_data, tgt_input_data, tgt_label_data = next(data_iter)
        except StopIteration:
            break

        src_batch = src_data.to(device)
        tgt_input = tgt_input_data.to(device)
        tgt_label = tgt_label_data.to(device)

        src_mask = create_padding_mask(src_batch, pad_idx_src)
        tgt_mask = create_look_ahead_mask(tgt_input, pad_idx_tgt)

        with torch.amp.autocast(device_type=device.type, enabled=(device.type == 'cuda')):

            # --- [!!!] è¿™æ˜¯æ­£ç¡®çš„â€œå¹¶è¡Œâ€è®­ç»ƒ (æ¥è‡ª train.py) [!!!] ---
            logits = model(src_batch, tgt_input, src_mask, tgt_mask)

            loss = loss_fn(
                logits.reshape(-1, logits.shape[-1]),
                tgt_label.reshape(-1)
            )
            # --- [!!!] å¹¶è¡Œè®­ç»ƒç»“æŸ [!!!] ---

            # --- [!!!] æ–°å¢ï¼šT=1 vs T>1 Loss è¯Šæ–­ (!!!] ---
            with torch.no_grad():
                loss_t1 = loss_fn(
                    logits[:, 0, :],  # (B, V)
                    tgt_label[:, 0]  # (B)
                )
                if logits.size(1) > 1:
                    loss_t_plus = loss_fn(
                        logits[:, 1:, :].reshape(-1, logits.shape[-1]),
                        tgt_label[:, 1:].reshape(-1)
                    )
                else:
                    loss_t_plus = torch.tensor(0.0)
            # --- [!!!] è¯Šæ–­ç»“æŸ [!!!] ---

        # (æ¥è‡ª train.py çš„æ ‡å‡†åå‘ä¼ æ’­)
        optimizer.zero_grad()
        # scaler.scale(loss).backward()
        # scaler.unscale_(optimizer)
        # torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
        # scaler.step(optimizer)
        # scaler.update()
        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
        optimizer.step()

        if lr_scheduler is not None:
            lr_scheduler.step()

        optimizer.zero_grad(set_to_none=True)
        total_loss_epoch += loss.item()

        # (!!!) æ³¨å…¥äº† T=1/T>1 å’Œ LR æ‰“å°çš„æ—¥å¿— (!!!)
        if (batch_count + 1) % log_interval == 0:
            # current_lr = lr_scheduler.get_last_lr()[0]
            print(
                f"  [Train] Epoch {epoch}, Batch {batch_count + 1}/{num_batches}, "
                # f"LR: {current_lr:.2e}, "
                f"Total Loss: {loss.item():.4e} "
                f"(T=1 Loss: {loss_t1.item():.2f}, T>1 Loss: {loss_t_plus.item():.4e})"
            )

        if (batch_count + 1) % validate_interval == 0:
            print(f"  (--- æ­£åœ¨è¿è¡ŒéªŒè¯ @ Batch {batch_count + 1} ---)")
            val_loss = evaluate(model, val_loader, loss_fn, device, pad_idx_src, pad_idx_tgt)
            model.train()  # (åˆ‡å›è®­ç»ƒæ¨¡å¼)
            print(f"  [Validate] Epoch {epoch}, Batch {batch_count + 1}, Val Loss: {val_loss:.4e}")

            if val_loss < best_val_loss:
                best_val_loss = val_loss
                print(f"    (æ–°ä½ç‚¹! Val Loss é™è‡³ {val_loss:.4e}ã€‚æ­£åœ¨ä¿å­˜ 'transformer_best_model.pth'...)")
                # (æˆ‘ä»¬åªä¿å­˜ state_dict, ä¿æŒä¸ train.py ä¸€è‡´)
                torch.save(model.state_dict(), "transformer_best_model.pth")

    return total_loss_epoch / num_batches, best_val_loss


# ----------------------------------------------------------------------
# ã€ä¸»ç¨‹åºã€‘: (æ¥è‡ª debug_app.py çš„è¿‡æ‹Ÿåˆ + è¯Šæ–­æµç¨‹)
# ----------------------------------------------------------------------

if __name__ == '__main__':

    DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"--- æ­£åœ¨ä½¿ç”¨è®¾å¤‡: {DEVICE} ---")

    # --- 1. è®¾ç½®è¶…å‚æ•° (æ¥è‡ª train.py, ä½†ç”¨äºè°ƒè¯•) ---
    BATCH_SIZE = 1  # (!!!) ä¿æŒ B=1 ä»¥ä¾¿è¿›è¡Œæ¸…æ™°çš„è¿‡æ‹Ÿåˆæµ‹è¯• (!!!)
    NUM_EPOCHS = 200  # (!!!) è¿è¡Œ 200 ä¸ª Epoch æ¥ç¡®ä¿èƒ½è¿‡æ‹Ÿåˆ (!!!)
    MAX_SEQ_LEN = 100
    LOG_INTERVAL = 5  # (!!!) å¢åŠ æ‰“å°é¢‘ç‡ (!!!)
    VALIDATE_INTERVAL = 20  # (!!!) å¢åŠ éªŒè¯é¢‘ç‡ (!!!)
    CHECKPOINT_FILE = "transformer_debug.pth"  # (ä½¿ç”¨æ–°çš„æ£€æŸ¥ç‚¹æ–‡ä»¶)

    print("\n--- [é˜¶æ®µä¸€] åŠ è½½æ•°æ®é›† (è¿‡æ‹Ÿåˆæµ‹è¯•æ¨¡å¼) ---")

    try:
        dataset_full = load_dataset('xmj2002/Chinese_modern_classical')
    except Exception as e:
        print(f"!! é”™è¯¯: æ— æ³•åŠ è½½ HuggingFace æ•°æ®é›†: {e}")
        sys.exit(1)

    # --- (!!!) æ³¨å…¥ debug_app.py çš„è¿‡æ‹Ÿåˆæ•°æ®æŠ½å– (!!!) ---
    print("=" * 60)
    print("!!! è­¦å‘Š: æ­£åœ¨æ‰§è¡Œ [è¿‡æ‹Ÿåˆæµ‹è¯•] æ¨¡å¼ !!!")
    try:
        overfit_data = dataset_full['train'].select(range(20))
        print(f"  (å·²æˆåŠŸæŠ½å– {len(overfit_data)} æ¡æ•°æ®ç”¨äºâ€œèƒŒè¯µâ€)")

        # (æˆ‘ä»¬ç”¨è¿™ 20 æ¡æ•°æ®æ—¢åšè®­ç»ƒåˆåšéªŒè¯)
        dataset_train_debug = overfit_data
        dataset_val_debug = overfit_data

    except Exception as e:
        print(f"!! é”™è¯¯: æŠ½å–æ•°æ®å¤±è´¥: {e}")
        sys.exit(1)

    # --- 2. å‡†å¤‡æ•°æ®å’Œæ¨¡å‹ ---
    train_loader, val_loader = setup_dataloaders(
        train_data=dataset_train_debug,
        val_data=dataset_val_debug,
        src_tokenizer=src_tokenizer,
        tgt_tokenizer=tgt_tokenizer,
        batch_size=BATCH_SIZE,
        max_allowed_len=MAX_SEQ_LEN
    )

    model, loss_fn, optimizer, start_epoch, best_val_loss, learning_rate = initialize_model(
        DEVICE,
        checkpoint_path=CHECKPOINT_FILE
    )

    #scaler = torch.amp.GradScaler(device=DEVICE.type, enabled=(DEVICE.type == 'cuda'))
    scaler = None

    print(f"\n--- [é˜¶æ®µäºŒ] å¼€å§‹è®­ç»ƒ (ä½¿ç”¨ train.py å¹¶è¡Œå¾ªç¯) ---")
    print(f"--- å°†ä» Epoch {start_epoch} å¼€å§‹è®­ç»ƒ (å…± {NUM_EPOCHS} ä¸ª) ---")

    for epoch in range(start_epoch, NUM_EPOCHS + 1):
        print(f"\n--- Epoch {epoch}/{NUM_EPOCHS} ---")

        train_loss, new_best_val_loss = train_one_epoch(
            model, train_loader, val_loader, loss_fn, optimizer,
            DEVICE, scaler, PAD_IDX_SRC, PAD_IDX_TGT,
            epoch=epoch,
            lr_scheduler=learning_rate,
            log_interval=LOG_INTERVAL,
            validate_interval=VALIDATE_INTERVAL,
            best_val_loss=best_val_loss
        )
        best_val_loss = new_best_val_loss
        print(f"--- Epoch {epoch} å¹³å‡è®­ç»ƒæŸå¤±: {train_loss:.4e} ---")

    print("\n--- [é˜¶æ®µä¸‰] è®­ç»ƒå®Œæˆ ---")
    print(f"æœ€å¥½çš„éªŒè¯é›† Loss: {best_val_loss:.4e}")

    # ------------------------------------------------------------------
    # (!!!) æ³¨å…¥ debug_app.py çš„â€œç»ˆæçŠ¶æ€æ£€æŸ¥â€ (!!!)
    # ------------------------------------------------------------------

    print("\n" + "=" * 70)
    print("--- [é˜¶æ®µå››] å¼€å§‹è¿è¡Œâ€œç»ˆæçŠ¶æ€æ£€æŸ¥â€ ---")
    print("=" * 70)

    try:
        # (1) å‡†å¤‡æ•°æ®
        print("  (1/4) å‡†å¤‡æµ‹è¯•æ•°æ® (æ¥è‡ªç¬¬ä¸€ä¸ª batch)...")
        src_batch, tgt_input_batch, tgt_label_batch = next(iter(train_loader))

        src_tensor = src_batch[0:1].to(DEVICE)
        tgt_tensor_full = tgt_input_batch[0:1].to(DEVICE)
        src_mask_const = create_padding_mask(src_tensor, PAD_IDX_SRC).to(DEVICE)

        test_sentence_classical = src_tokenizer.decode(src_batch[0].tolist())
        expected_modern = tgt_tokenizer.decode(tgt_label_batch[0].tolist())

        print(f"    æµ‹è¯•è¾“å…¥ (æ–‡è¨€æ–‡): > {test_sentence_classical}")
        print(f"    æœŸæœ›è¾“å‡º (ç°ä»£æ–‡): > {expected_modern}")

        # (2) è¿è¡Œ T=1 å¥åº·è¯Šæ–­ (æ€»æ˜¯åœ¨ eval æ¨¡å¼)
        print("\n  (2/4) è¿è¡Œ [diagnose_model_health] (T=1 æ£€æŸ¥)...")
        diagnose_model_health(model, test_sentence_classical, DEVICE)
        print("\n" + "=" * 40)
        print("ğŸ•µï¸â€â™‚ï¸ è‡´å‘½çº¿ç´¢æ£€æŸ¥ (TRAIN_UTILS & DATA)")
        print("=" * 40)

        # 1. æ£€æŸ¥ PAD ID ç©¶ç«Ÿæ˜¯å¤šå°‘
        real_pad_id = src_tokenizer.token_to_id("<pad>")
        print(f"Tokenier <pad> ID: {real_pad_id}")
        print(f"TrainUtils é»˜è®¤ PAD_IDX: 1")
        if real_pad_id != 1:
            print("ğŸš¨ è­¦å‘Š: ID ä¸åŒ¹é…ï¼mask = (seq == 1) å¯èƒ½å…¨é”™ï¼")

        # 2. æ£€æŸ¥ Src Mask çš„å®é™…å†…å®¹ (Cross Attention çš„å…³é”®)
        # é‡æ–°ç”Ÿæˆä¸€ä¸ª mask
        test_src_mask = create_padding_mask(src_tensor, real_pad_id)
        print(f"Src Mask Shape: {test_src_mask.shape}")  # åº”è¯¥æ˜¯ (1, 1, 1, S)
        # æ‰“å°å‰ 20 ä¸ª mask å€¼ (True=å±è”½, False=ä¿ç•™)
        mask_vals = test_src_mask[0, 0, 0, :20].tolist()
        print(f"Src Mask (å‰20): {mask_vals}")

        # 3. é€»è¾‘åˆ¤æ–­
        if all(mask_vals):
            print("âŒ é”™è¯¯: Mask å…¨ä¸º Trueï¼Encoder è¢«å®Œå…¨å±è”½ï¼Œæ¨¡å‹å…¨ç›²ï¼")
            print("   åŸå› : è¾“å…¥æ•°æ®å…¨æ˜¯ Padï¼Œæˆ–è€… pad_idx æé”™äº†ã€‚")
        elif not any(mask_vals) and 1 in src_tensor[0].tolist():
            print("âŒ é”™è¯¯: Mask å…¨ä¸º Falseï¼Œä½†è¾“å…¥é‡Œæ˜æ˜æœ‰ Pad (ID=1)ï¼")
            print("   åŸå› : create_padding_mask ç”¨äº†é”™è¯¯çš„ pad_idxã€‚")
        else:
            print("âœ… Mask çœ‹èµ·æ¥æ­£å¸¸ (æ··åˆäº† T/F)ã€‚")
        print("=" * 40 + "\n")

        # (3) è¿è¡Œâ€œé€æ—¶é—´æ­¥â€æ£€æŸ¥ (åœ¨ EVAL æ¨¡å¼)
        print("\n  (3/4) è¿è¡Œ [test_autoregressive_steps] (åœ¨ EVAL æ¨¡å¼)...")
        model.eval()  # (!!!) å¼ºåˆ¶ EVAL æ¨¡å¼ (!!!)
        acc_eval = test_autoregressive_steps(
            model=model,
            src_tensor=src_tensor,
            tgt_tensor=tgt_tensor_full,
            src_mask=src_mask_const,
            pad_idx_tgt=PAD_IDX_TGT,
            create_look_ahead_mask_fn=create_look_ahead_mask,
            device=DEVICE
        )

        # (4) è¿è¡Œâ€œé€æ—¶é—´æ­¥â€æ£€æŸ¥ (åœ¨ TRAIN æ¨¡å¼)
        print("\n  (4/4) è¿è¡Œ [test_autoregressive_steps] (åœ¨ TRAIN æ¨¡å¼)...")
        model.train()  # (!!!) å¼ºåˆ¶ TRAIN æ¨¡å¼ (!!!)
        acc_train = test_autoregressive_steps(
            model=model,
            src_tensor=src_tensor,
            tgt_tensor=tgt_tensor_full,
            src_mask=src_mask_const,
            pad_idx_tgt=PAD_IDX_TGT,
            create_look_ahead_mask_fn=create_look_ahead_mask,
            device=DEVICE
        )

        # (5) æœ€ç»ˆè¯Šæ–­
        print("\n" + "=" * 70)
        print("--- [ç»ˆæè¯Šæ–­ç»“æœ] ---")
        print(f"  EVAL æ¨¡å¼ (é€T) å‡†ç¡®ç‡: {acc_eval:.2f}%")
        print(f"  TRAIN æ¨¡å¼ (é€T) å‡†ç¡®ç‡: {acc_train:.2f}%")

        if acc_eval < 100.0 and acc_train > 99.0:
            print("\n  (!!!!!!) ğŸš¨ è¯Šæ–­å‘½ä¸­! ğŸš¨ (!!!!!!)")
            print("  æ¨¡å‹åœ¨ EVAL æ¨¡å¼ä¸‹å¤±è´¥ï¼Œä½†åœ¨ TRAIN æ¨¡å¼ä¸‹æˆåŠŸã€‚")
            print("  è¿™ 100% è¯æ˜äº† Bug å‡ºåœ¨æŸä¸ªåœ¨ train/eval æ¨¡å¼ä¸‹")
            print("  è¡Œä¸ºä¸ä¸€è‡´çš„æ¨¡å— (ä¾‹å¦‚ Dropout æˆ– LayerNorm)ã€‚")
        elif acc_eval > 99.0:
            print("\n  (ğŸ‰) æ­å–œ! æ¨¡å‹åœ¨ EVAL æ¨¡å¼ä¸‹é€šè¿‡äº†æµ‹è¯•ã€‚")
            print("  â€œåªè¾“å‡º EOSâ€ çš„ Bug ä¼¼ä¹å·²è§£å†³ã€‚")
        else:
            print("\n  (???) è¯Šæ–­æœªå‘½ä¸­ã€‚")
            print("  æ¨¡å‹åœ¨ TRAIN å’Œ EVAL æ¨¡å¼ä¸‹å‡å¤±è´¥ã€‚")
            print("  è¿™è¯´æ˜æ¨¡å‹æ ¹æœ¬æ²¡æœ‰â€œèƒŒè¯µâ€æˆåŠŸ (Loss æœªé™åˆ° 0)ã€‚")
        print("=" * 70)

        # (6) è¿è¡Œè´ªå©ªç¿»è¯‘ (å®ƒä¼šå¼ºåˆ¶ model.eval())
        translation = translate_greedy(
            model=model,
            src_sentence=test_sentence_classical,
            src_tokenizer=src_tokenizer,
            tgt_tokenizer=tgt_tokenizer,
            device=DEVICE,
            pad_idx_src=PAD_IDX_SRC,
            pad_idx_tgt=PAD_IDX_TGT,
            sos_idx_tgt=SOS_IDX_TGT,
            eos_idx_tgt=EOS_IDX_TGT,
            max_len=150
        )

        print(f"\n  (!!!) æœ€ç»ˆå®é™…ç¿»è¯‘ç»“æœ (EVAL æ¨¡å¼) (!!!):")
        print(f"  > {translation}")

    except Exception as e:
        print(f"\n  (!!!!!!) ğŸš¨ [ç»ˆæçŠ¶æ€æ£€æŸ¥] å‡†å¤‡æˆ–è¿è¡Œæ—¶å¤±è´¥: {e} (!!!!!!)")
        import traceback

        traceback.print_exc()

    print("\n--- è°ƒè¯•è„šæœ¬è¿è¡Œç»“æŸ ---")
